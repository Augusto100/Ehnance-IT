# -*- coding: utf-8 -*-
"""Proyect Cleaning Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vNIQQe9veGN4wSDG2Q3aX9gNT8pYxVaE

#### Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# %config InlineBackend.figure_format = 'svg'
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
plt.style.use('seaborn')
import seaborn as sns
import pandas as pd
import numpy as np
import datetime

"""### Getting data from Google-Drive

"""

data=pd.read_excel('/content/drive/MyDrive/Enhance it/Cesar Perez - raw_house_data.csv.xlsx')

"""## Knowing the data


"""

data.head()

data.describe().round(2)

#Describe non numerical features
data.describe(include="all").round(2)

"""### Data types """

data.info()

"""### Null values"""

data.isnull().sum()

"""**All null values**"""

data.isnull().sum().sum()

"""### Features with object type values"""

data.loc[:, data.dtypes == object]

"""* Later we will see more in detail why these columns are of the object type
* The variable bathrooms would be expected to be of type integer or maybe float but we can see that they have values ​​with type datetime.datime

### Which columns have None values and how many values are?
"""

dict={"Feature":[],"None":[]}
for i in data.columns:
  dict["Feature"].append(i)
  dict["None"].append(len((data[data[i]=='None'][i])))
pd.DataFrame(dict).sort_values(["None"],ascending=False)

"""* Later we will see if this None values are null or represent the value zero

### Function to detect outliers
"""

def find_outliers(data, column):
  percentiles=data[column].describe()
  Q1,Q3=percentiles["25%"], percentiles["75%"]
  IQR=Q3-Q1
  return(data[(data[column]<Q1-1.5*IQR) | (data[column]>Q3+1.5*IQR)])

"""# Analysis by feature

## MLS
"""

len(np.unique(data["MLS"]))

float_columns=data.select_dtypes(include=['float'])

data["MLS"].describe().round(2)

sns.heatmap(float_columns.corr(), annot=True);

"""* For this feature we can conclude that this feature is a key(id) because every values is unique
* We can drop this value, 
because the heatmap shows that there is only a slight correlation with the variable zip code

## Sold price
"""

pd.DataFrame(data["sold_price"]).head(7)

sns.boxplot(x="sold_price", data=data);

"""* There is <mark>no</mark> `null` or `NONE` values"""

sns.boxplot(y="sold_price", data=data, showfliers = False);

"""### How many outliers are ?

"""

Q1=data["sold_price"].describe().round(2)[4]
Q3=data["sold_price"].describe().round(2)[6]
IQ=Q3-Q1

low_out, high_out=Q1-(1.5*IQ),Q3+(1.5*IQ)

outliers = data[(data["sold_price"]<low_out) | (data["sold_price"]>high_out)]

outliers.shape[0]

outliers.sort_values(["sold_price"]).head(5)

from numpy.lib.function_base import percentile
Q1=np.percentile(data["sold_price"],25)
Q3=np.percentile(data["sold_price"],75)
IQ=Q3-Q1

Q2=np.percentile(data["sold_price"],25)
Q2

"""### Boxplot"""

import plotly.express as px
px.box(data, y="sold_price",points=False)

px.box(data, y="sold_price")

"""### Histograma"""

px.histogram(data,x="sold_price")

"""## Zip code

* Even though `zip code` is a `float type` variable, it should be treated as `categorical `because it has no `order` and incrementing or decrementing by one unit makes no sense
"""

np.unique(data["zipcode"])

data['zipcode'] = data['zipcode'].astype(float).astype(int).astype('string')

data["zipcode"]=data["zipcode"].astype("category")

dummies_zip=pd.get_dummies(data["zipcode"], prefix="Zip")
data=data.join(dummies_zip)

data=pd.get_dummies(data, columns=["zipcode"], prefix="Zip",drop_first=True)

"""## Loingitude and latitude

"""

data[["longitude", "latitude"]].describe(percentiles=[0.25,0.5,0.75])

"""* Outliers"""

find_outliers(data,"latitude").shape

find_outliers(data,"longitude").shape

sns.boxplot( y="latitude", data=data, showfliers=False);

"""Although a large number of `outliers` can be observed for these two variables, they are known to have `no order`, so no `amputation` will be performed.

## year_built
### Building age
* Instead of using the year of construction `(year_built)`, the difference between the most recent date of construction (`2019`)and the year of construction of the house will be used.
* The new feature will have the name `building_age`

* This is because if the variable is taken as a reference, the `scale` would be too large to note the difference between the years of the home's age
"""

sns.boxplot(x="year_built", data=data, color = 'skyblue',showfliers = False );

sns.boxplot(x="year_built", data=data);

"""* There are some records that are outliers(`see boxplot`), it could be that the `building age` have been recorded instead of the year it was built(`year_built`).

* It will be checked if only the `6` records that have `year_built=0` are all that exist and will be replaced first with the year `2019`
"""

#Looking for outliers
data[data["year_built"]<1893]

#Looking for outliers
data[data["year_built"]<1900]

# Commented out IPython magic to ensure Python compatibility.
# #@title
# 
# #@title˚
# %%html magic ˚
# <div style="background-color: #ddffdd;border-left: 6px solid #04AA6D ;width=4px; margin-bottom=22px;margin-bottom: 10px;
#   padding: 5px 12px;">
#   <h3>
#   <strong style="color:green">Conclusion</strong> </h3>
#   <li>It is concluded that the oldest house is from <strong>1893</strong> and although it is an outlier it is believed that these data point is <strong>true</strong>
#   <li>  Finally, only the <strong>5</strong> values ​​that are <strong>zero</strong> will be replaced by the value <strong>2019</strong>
# </div>

##Max date
max_building_age=max(data["year_built"])
# Replace 0's by 2019
data["year_built"]=np.where(data["year_built"]==0,2019,data["year_built"])
##New feature
data["build_age"]=max_building_age-data["year_built"]

data["build_age"].describe()

data["year_built"].describe()

sns.boxplot(data=data, x="build_age");

sns.boxplot(data=data, x="year_built");

# Commented out IPython magic to ensure Python compatibility.
# #@title
# %%html magic 
# <div style="background-color: #ddffdd;border-left: 6px solid #04AA6D ;width=4px; margin-bottom=22px;margin-bottom: 10px;
#   padding: 5px 12px;">
#   <h3>
#   <strong style="color:green">Conclusion</strong> </h3>
#   <li>It can be seen that the distribution of the two boxplots are symmetric but the scale is <span style="color:green"><strong>better</strong></span> for future predictions.
# </div>
#

"""## Bedrooms

"""

data["bedrooms"].describe()



## Number of outliers
find_outliers(data,"bedrooms").shape[0]

sns.barplot(y="sold_price",x="bedrooms",data=data);

df_bedrooms=data["bedrooms"].value_counts()

sns.barplot(df_bedrooms.index,df_bedrooms.values );

"""Podemos ver que hay una tendencia a la alza entre el numero de habitaciones y el precio de venta para valores entre 2 y 7. Por otro lados esta tendencia no se mantiene, para viviendas con numero de baños mayores a 7, sin embargo en el barplot de abajo podemos ver que hay muy pocos valores de viviendas con 8 baños o más por lo que se haraá un análisis amputando estos outliers(bedrooms>8)"""

find_outliers(data,"bedrooms")

outliers_bedrooms=np.unique(find_outliers(data, "bedrooms")["bedrooms"].values)

data_no_outlieres_bedrooms=data[~data["bedrooms"].isin(outliers_bedrooms)][["bedrooms","sold_price"]]

data_no_outlieres_bedrooms.corr()

sns.boxplot(data=data_no_outlieres_bedrooms, x="bedrooms", y=data_no_outlieres_bedrooms.index)

data.iloc[:,0:15].info()

"""#Analysis for features with `type=object`

#### Function that counts how many types of data we have in a specific column
"""

def count_types(column):
  float_type=0
  string_type=0
  date_type=0
  #df={"Feature":[],"float_type":[],"date_type":[],"string_type":[],"Total":[]}
  for value in data[column]:
    if type(value)==float:
      float_type=float_type+1
    elif type(value)==str:
      string_type=string_type+1
    else: 
      date_type=date_type+1
  return(print("Column: ",column,", float_type: ",float_type, ", date_type: ",date_type,", string_type: ",string_type,", Total: ",float_type+string_type+date_type))

def count_types(data,column):
  float_type=0
  string_type=0
  date_type=0
  #df={"Feature":[],"float_type":[],"date_type":[],"string_type":[],"Total":[]}
  for value in data[column]:
    if type(value)==float:
      float_type=float_type+1
    elif type(value)==str:
      string_type=string_type+1
    else: 
      date_type=date_type+1
  return(print("Column: ",column,", float_type: ",float_type, ", date_type: ",date_type,", string_type: ",string_type,", Total: ",float_type+string_type+date_type))

for i in data.columns:
  print(count_types(data, i))

# Commented out IPython magic to ensure Python compatibility.
# #@title
# #@title˚
# %%html magic
# <div style="background-color: #ffdddd;border-left: 6px solid #f44336 ; margin-bottom=22:pxmargin-bottom: 15px;
#   padding: 4px 12px;">
#   <strong style="color:#f44336">Warning</strong> <p>
#   This variables have <span style="color:red">three </span> different types <span style="color:red"><strong>lot_acress, taxes, year_built, bathrooms, sqrt_ft, garage, HOA</strong></span></p>
# </div>

data[["lot_acres","taxes","year_built","bathrooms","sqrt_ft","garage","HOA"]]

"""## lot_acres"""

df=data._convert(numeric=True)

df["lot_acres"].describe()

median_lot_acres=df["lot_acres"].describe()["50%"]

mean_lot_acres=df["lot_acres"].describe()["mean"]

median_lot_acres, mean_lot_acres

count_types(data,"lot_acres")

#Changing None and datetime values with 0
data["lot_acres"]=np.where(~data["lot_acres"].isin([x for x in data["lot_acres"] if isinstance(x, float)]),median_lot_acres, data["lot_acres"] ).astype(float)

sns.boxplot(x="lot_acres",data=data);

data.lot_acres.describe()

find_outliers(data, "lot_acres").shape



"""## taxes

"""

mean_taxes=df["taxes"].describe()["mean"]

count_types(data, "taxes")

#Changing None and datetime values with 0
data["taxes"]=np.where(~data["taxes"].isin([x for x in data["taxes"] if isinstance(x, float)]),mean_taxes, data["taxes"] ).astype(float)

find_outliers(data, "taxes").shape[0]

"""* There are `756` outliers"""

find_outliers(data, "taxes")

sns.boxplot(x="taxes", data=data);

sns.boxplot(x="taxes", data=data, showfliers=False);



# Commented out IPython magic to ensure Python compatibility.
# #@title
# %%html magic 
# <div style="background-color: #e7f3fe;border-left: 6px solid #2196F3 ;width=4px; margin-bottom=22px;margin-bottom: 10px;
#   padding: 5px 12px;">
#   <h3>
#   <strong style="color:blue">Info</strong> </h3>
#   <li>After drop the values that with <strong>type=datetime.datetime</strong> there are <span style="color:blue">4362</span> records.
#   <li> After convert the string values to float values and compute linear correlation, we can see that the linear correlation <span style="color:blue">is not significant</span> for some other numerical variable.
#   <li> So we can <span style="color:blue">drop</span> this variable or <span style="color:blue">impute</span> the values ​​with data type equal to <span style="color:blue">datetime.datetime to the mean</span>
# </div>

"""## bathrooms



"""

count_types(data,"bathrooms")

"""* We only have `17` data that is not of `float type`. So we can review these cases in detail and make a decision

"""

data[~data["bathrooms"].isin([x for x in data["bathrooms"] if isinstance(x,float)])]["bathrooms"]

data[data["bathrooms"]==0]

"""* We can see that there are no register with `bathrooms==0`.
* We can replace the `None` values ​​with zero and the `datatime.datatime` values ​​with the mean 
* Since there are so few values, we might as well `delete` them.

"""

data2=data[data["bathrooms"].isin([x for x in data["bathrooms"] if isinstance(x,float)])]

data2["bathrooms"]=data2["bathrooms"].astype(float)

"""### Outliers

"""

#data2.drop(["bathrrooms"],axis=1)

find_outliers(data2,"bathrooms").shape

"""### Boxplot

#### With outliers
"""

sns.boxplot(data=data2, y="bathrooms");

"""#### With no outliers"""

sns.boxplot(data=data2, y="bathrooms", showfliers = False);

"""### Barplots"""

sns.barplot(y="sold_price",x="bathrooms",data=data2);

df_bathrooms=data2["bathrooms"].value_counts()

data2[["sold_price","bathrooms"]].corr()

outliers_bathroom=np.unique(find_outliers(data2, "bathrooms")["bathrooms"].values)
outliers_bathroom

# Linear correlation with no outliers
data2[~data2["bathrooms"].isin(outliers_bathroom)][["bathrooms","sold_price"]].corr()

sns.barplot(df_bathrooms.index,df_bathrooms.values );

sns.histplot(df_bathrooms)

"""## sqrt_ft"""

count_types(data2,"sqrt_ft")

"""* There are `51` non `float` values"""

count_types(data,"lot_acres")

data2[~data2["sqrt_ft"].isin(x for x in data2["sqrt_ft"] if isinstance(x,float))]["sqrt_ft"].values

min(df["sqrt_ft"])

"""* 50 values are "None" and 1 values is `datetime`
* We can see that there are non zero values and the  min value is `6396`. So in this case it could not be `intuited` that the value `None` 
represents the value `0`.
* We will replace "None" and "datetime" values by the mean

"""

mean_sqrt_ft=data2._convert(numeric=True)["sqrt_ft"].describe()["mean"]

#Replace NONE and datetime values
data2["sqrt_ft"]=np.where(~data2["sqrt_ft"].isin([x for x in data2["sqrt_ft"] if isinstance(x, float)]),mean_sqrt_ft, data2["sqrt_ft"] ).astype(float)

find_outliers(data2, "sqrt_ft").shape

sns.boxplot(data=data2, x="sqrt_ft")

sns.boxplot(data=data2, x="sqrt_ft", showfliers=False )

data2[["sold_price", "sqrt_ft"]].corr()

df[["sold_price","sqrt_ft"]].corr()

"""* With the imputations the correlation changed significantly

"""

plt.scatter(x=data2["sqrt_ft"], y=data2["sold_price"])
plt.xlabel("sqrt_ft")
plt.ylabel("sold_price") ;

"""## Garage"""

count_types(data2, "garage")

data2[~data2["garage"].isin(x for x in data2["garage"] if isinstance(x,float))]["garage"].values

data2[~data2["garage"].isin([x for x in data2["garage"] if isinstance(x,float)])].iloc[:,1:15]

x=pd.DataFrame()

data3=data2[data2["garage"].isin([x for x in data2["garage"] if isinstance(x,float)])]

data3["garage"]=data3["garage"].astype(float)

sns.boxplot(x="garage", data=data3);

sns.boxplot(x="garage", data=data3, showfliers=False );

find_outliers(data3,"garage").shape[0]

data3[["sold_price","garage"]].corr()

df[["sold_price","garage"]].corr()

"""## kitchen_features"""

count_types(data3, "kitchen_features")

"""* Looking for all distinct values"""

y=[]
for i in data3["kitchen_features"]:
  for j in i.split(","):
    y.append(j)

len(np.unique(y))

def clean_alt_list(list_):
    list_ = list_.replace(', ', '","')
    list_ = list_.replace('[', '["')
    list_ = list_.replace(']', '"]')
    return list_

kitchen_features = {}
for i in data3["kitchen_features"]:
    for j in j:
        if j not in kitchen_features:
            kitchen_features[j] = 1
        else:
            kitchen_features[j] += 1

x={"kitchen_feature":[],"Count":[]}
for i in np.unique(y):
  x["kitchen_feature"].append(i)
  x["Count"].append(y.count(i))

kitchen_feature=pd.DataFrame(x).sort_values(["Count"],ascending=False)

kitchen_feature.head(5)

sns.barplot(x="kitchen_feature",y="Count", data=kitchen_feature[kitchen_feature["Count"]>100])
plt.xticks(rotation=90);

"""* We just look for the values that at least appear `500` times(10% total data)
* We get `12`values
"""

sns.barplot(x="kitchen_feature",y="Count", data=kitchen_feature[kitchen_feature["Count"]>500])
plt.xticks(rotation=90);

tweleve_best_kitchen_features=kitchen_feature[kitchen_feature["Count"]>500]

dummie=pd.DataFrame()
dummie["dummie"]=data3["kitchen_features"].str.split(',')

def boolean_df(item_lists, unique_items):
# Create empty dict
    bool_dict = {}
    
    # Loop through all the tags
    for i, item in enumerate(unique_items):
        
        # Apply boolean mask
        bool_dict[item] = item_lists.apply(lambda x: item in x)
            
    # Return the results as a dataframe
    return pd.DataFrame(bool_dict)

#dummies_kitchen = boolean_df(dummie["dummie"], tweleve_best_kitchen_features.kitchen_feature.unique())

dummies_kitchen=boolean_df(data3["kitchen_features"], tweleve_best_kitchen_features.kitchen_feature.unique())

data3=data3.join(dummies_kitchen)

data3.head(5)

"""## floor_covering

"""

count_types(data3, "floor_covering")

"""* Looking for all distinct values"""

data3[["floor_covering","kitchen_features"]]

data3

y=[]
for i in data3["floor_covering"]:
  for j in i.split(","):
    y.append(j)

len(np.unique(y))

floor_covering = {}
for i in data3["floor_covering"]:
    for j in j:
        if j not in kitchen_features:
            kitchen_features[j] = 1
        else:
            kitchen_features[j] += 1

x={"floor_coverting":[],"Count":[]}
for i in np.unique(y):
  x["floor_coverting"].append(i)
  x["Count"].append(y.count(i))

floor_coverting=pd.DataFrame(x).sort_values(["Count"],ascending=False)

floor_coverting.head(5)

sns.barplot(x="floor_coverting",y="Count", data=floor_coverting[floor_coverting["Count"]>100])
plt.xticks(rotation=90);

"""* We just look for the values that at least appear `500` times(10% total data)
* We get `12`values
"""

sns.barplot(x="floor_coverting",y="Count", data=floor_coverting[floor_coverting["Count"]>500])
plt.xticks(rotation=90);

five_best_floor_coverting_feature=floor_coverting[floor_coverting["Count"]>500]

five_best_floor_coverting_feature

five_best_floor_coverting_feature.floor_coverting.unique()

dummies_floor_coverting=boolean_df(data3["floor_covering"], five_best_floor_coverting_feature.floor_coverting.unique())

data3=data3.join(dummies_floor_coverting)

data3.head(5)

"""## HOA"""

count_types(data3,"HOA")

"""* We can see that this feature only has 6 values with `date_type=datatime.datatime`
* 

"""

data3[data3["HOA"]=="None"].shape[0]

sns.boxplot(data=df, y="HOA");

#Filter HOA feature by datetype string
HOA=[x for x in data["HOA"] if isinstance(x, str)]

#Create new df (HOA_df) just with string values
HOA_df=data[data['HOA'].isin(HOA)]

HOA_df.head(5)

"""* We can see that there are a lot of `None` values 
* 
"""

HOA_df[HOA_df["HOA"]!="None"]["HOA"].values

"""* We can see that the strings are just numerics values parsed to string"""

df["HOA"].describe()

HOA_float=[x for x in data3["HOA"] if isinstance(x, float)]

HOA_float_df=data3[data3['HOA'].isin(HOA_float)]["HOA"].astype(float)

HOA_float_df.describe().round(2)

"""* We can see that the values ​​with type string other than `None` have approximately the same `means` and `variances`, so they will be converted to `numeric` values ​​and for now the `None `values ​​will be replaced by the `mean` of the observations and another column will be created where will replace the `None` values ​​with `0`"""

HOA_string_NONE=[x for x in data["HOA"] if isinstance(x, str) and x!="None"]

##Replace string values ​​other than "None" by their numeric value
data3["HOA"]=np.where(data3["HOA"].isin(HOA_string_NONE),data3["HOA"].isin(HOA_string_NONE).astype(float), data3["HOA"])

count_types(data3,"HOA")

"""* We can see that now we have less string values


---

We can use the `.convert(numeric=True)` method to get the `mean` of the `HOA` feature. This method convert all data to `numeric` type , in this case the non-numeric values, `string` and `None` will be converted to `0`. So we can get the `mean` value of this clean table to replace the `None` and `datetime.datetime` values by this `mean`
"""

mean_HOA=data3._convert(numeric=True)["HOA"].describe()["mean"]
mean_HOA

data3["HOA"]=np.where(~data3["HOA"].isin([x for x in data3["HOA"] if isinstance(x,float)]),mean_HOA, data3["HOA"]).astype(float)

data3["HOA"].describe()

plt.scatter(data3["sold_price"],data3["HOA"]);

data3[["sold_price","HOA"]].corr()